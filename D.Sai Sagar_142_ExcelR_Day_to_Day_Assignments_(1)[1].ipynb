{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34b9360",
   "metadata": {},
   "source": [
    "### 2211CS020142\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e44931",
   "metadata": {},
   "source": [
    "### Day-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12012eb4",
   "metadata": {},
   "source": [
    "### 1.Jupyter Notebook and Data Types\n",
    "Write a program in Jupyter Notebook to declare variables of different data types (integer, float, string, and boolean). Print each variable and its type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65dfc4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Variable:\n",
      "Value: 10, Type: <class 'int'>\n",
      "\n",
      "Float Variable:\n",
      "Value: 3.14, Type: <class 'float'>\n",
      "\n",
      "String Variable:\n",
      "Value: Hello, World!, Type: <class 'str'>\n",
      "\n",
      "Boolean Variable:\n",
      "Value: True, Type: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "integer_var = 10    \n",
    "float_var = 3.14       \n",
    "string_var = \"Hello, World!\"  \n",
    "boolean_var = True    \n",
    "\n",
    "print(\"Integer Variable:\")\n",
    "print(f\"Value: {integer_var}, Type: {type(integer_var)}\\n\")\n",
    "\n",
    "print(\"Float Variable:\")\n",
    "print(f\"Value: {float_var}, Type: {type(float_var)}\\n\")\n",
    "\n",
    "print(\"String Variable:\")\n",
    "print(f\"Value: {string_var}, Type: {type(string_var)}\\n\")\n",
    "\n",
    "print(\"Boolean Variable:\")\n",
    "print(f\"Value: {boolean_var}, Type: {type(boolean_var)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745ec58",
   "metadata": {},
   "source": [
    "### Day-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc5598",
   "metadata": {},
   "source": [
    "### Create a List, tuple and Dictionary with 5 elements in it and how to access few elements based on the index. Try  with different examples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9d3f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing List Elements:\n",
      "First Element: 10\n",
      "Third Element: 30\n",
      "Last Element: 50\n",
      "\n",
      "Accessing Tuple Elements:\n",
      "Second Element: banana\n",
      "Fourth Element: date\n",
      "\n",
      "Accessing Dictionary Elements:\n",
      "Value of Key 'a': 1\n",
      "Value of Key 'c': 3\n",
      "Value of Key 'e': 5\n",
      "\n",
      "Slicing List and Tuple:\n",
      "Slice of List (index 1 to 3): [20, 30, 40]\n",
      "Slice of Tuple (index 0 to 2): ('apple', 'banana', 'cherry')\n"
     ]
    }
   ],
   "source": [
    "my_list = [10, 20, 30, 40, 50]  \n",
    "my_tuple = (\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\")  \n",
    "my_dict = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5}  \n",
    "\n",
    "print(\"Accessing List Elements:\")\n",
    "print(f\"First Element: {my_list[0]}\")\n",
    "print(f\"Third Element: {my_list[2]}\")\n",
    "print(f\"Last Element: {my_list[-1]}\\n\")\n",
    "\n",
    "print(\"Accessing Tuple Elements:\")\n",
    "print(f\"Second Element: {my_tuple[1]}\")\n",
    "print(f\"Fourth Element: {my_tuple[3]}\\n\")\n",
    "\n",
    "\n",
    "print(\"Accessing Dictionary Elements:\")\n",
    "print(f\"Value of Key 'a': {my_dict['a']}\")\n",
    "print(f\"Value of Key 'c': {my_dict['c']}\")\n",
    "print(f\"Value of Key 'e': {my_dict['e']}\\n\")\n",
    "\n",
    "\n",
    "print(\"Slicing List and Tuple:\")\n",
    "print(f\"Slice of List (index 1 to 3): {my_list[1:4]}\")\n",
    "print(f\"Slice of Tuple (index 0 to 2): {my_tuple[0:3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893879cc",
   "metadata": {},
   "source": [
    "### Day-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092ea26",
   "metadata": {},
   "source": [
    "### Write a Python program that takes a student's marks in three subjects as input.\n",
    "### If the average is greater than or equal to 90, print \"Grade: A\".\n",
    "### If the average is between 80 and 89, print \"Grade: B\".\n",
    "### If the average is between 70 and 79, print \"Grade: C\".\n",
    "### Otherwise, print \"Grade: Fail\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983da931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter marks for subject 1: 85\n",
      "Enter marks for subject 2: 90\n",
      "Enter marks for subject 3: 80\n",
      "Average: 85.00\n",
      "Grade: B\n"
     ]
    }
   ],
   "source": [
    "mark1 = int(input(\"Enter marks for subject 1: \"))\n",
    "mark2 = int(input(\"Enter marks for subject 2: \"))\n",
    "mark3 = int(input(\"Enter marks for subject 3: \"))\n",
    "average = (mark1 + mark2 + mark3) / 3\n",
    "if average >= 90:\n",
    "    grade = \"Grade: A\"\n",
    "elif 80 <= average < 90:\n",
    "    grade = \"Grade: B\"\n",
    "elif 70 <= average < 80:\n",
    "    grade = \"Grade: C\"\n",
    "else:\n",
    "    grade = \"Grade: Fail\"\n",
    "print(f\"Average: {average:.2f}\")\n",
    "print(grade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ad595",
   "metadata": {},
   "source": [
    "### Day-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3d6b6",
   "metadata": {},
   "source": [
    "### Write a Python program to calculate the sum of all even numbers between 1 and a given positive integer n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112d6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a positive integer: 12\n",
      "The sum of all even numbers between 1 and 12 is: 42\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter a positive integer: \"))\n",
    "\n",
    "even_sum = sum(i for i in range(2, n+1, 2))\n",
    "\n",
    "print(f\"The sum of all even numbers between 1 and {n} is: {even_sum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52436932",
   "metadata": {},
   "source": [
    "### Day-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22491695",
   "metadata": {},
   "source": [
    "### 1. Write a Python program to calculate the frequency of each word in a given text. Print the\n",
    "### words and their corresponding counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8ff5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies:\n",
      "this: 2\n",
      "is: 2\n",
      "a: 1\n",
      "sample: 1\n",
      "text: 3\n",
      "used: 1\n",
      "to: 1\n",
      "calculate: 1\n",
      "the: 3\n",
      "frequency: 1\n",
      "of: 1\n",
      "each: 1\n",
      "word: 1\n",
      "in: 1\n",
      "some: 1\n",
      "words: 1\n",
      "may: 1\n",
      "repeat: 1\n",
      "and: 1\n",
      "program: 1\n",
      "will: 1\n",
      "count: 1\n",
      "them: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def calculate_word_frequency(text):\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "   \n",
    "    word_count = Counter(words)\n",
    "    \n",
    "\n",
    "    print(\"Word Frequencies:\")\n",
    "    for word, count in word_count.items():\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    This is a sample text. This text is used to calculate the frequency of each word in the text. \n",
    "    Some words may repeat, and the program will count them.\n",
    "    \"\"\"\n",
    "    calculate_word_frequency(sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d500d9",
   "metadata": {},
   "source": [
    "### Day-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7844b94",
   "metadata": {},
   "source": [
    "### Write a Python program to using NLTK and Spacy\n",
    "### Convert text to lowercase.\n",
    "### Remove stopwords using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8799848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text Tokens: ['\\n    ', 'sample', 'text', 'demonstrate', 'use', 'nltk', 'spacy', 'processing', 'natural', 'language', '\\n    ', 'program', 'remove', 'stopwords', 'convert', 'text', 'lowercase', '\\n    ']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "def process_text(text):\n",
    "\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "   \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "  \n",
    "    doc = nlp(text_lower)\n",
    "  \n",
    "    processed_tokens = [token.text for token in doc if token.text not in nltk_stopwords and not token.is_punct]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    This is a sample text to demonstrate the use of NLTK and spaCy in processing natural language. \n",
    "    The program will remove stopwords and convert the text to lowercase.\n",
    "    \"\"\"\n",
    "    processed = process_text(sample_text)\n",
    "    print(\"Processed Text Tokens:\", processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bca660",
   "metadata": {},
   "source": [
    "### Day-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41678a",
   "metadata": {},
   "source": [
    "### Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "272e53c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 Preprocessed Tokens: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'sentenc', 'often', 'use', 'type', 'exercis', 'demonstr', 'font', 'contain', 'everi', 'letter', 'english', 'alphabet', 'howev', 'realm', 'natur', 'languag', 'process', 'anoth', 'text', 'analyz', 'data', 'scientist', 'often', 'preprocess', 'text', 'remov', 'stopword', 'stem', 'lemmat', 'step', 'help', 'simplifi', 'datum', 'make', 'easy', 'machin', 'learn', 'algorithm', 'find', 'pattern', 'make', 'predict', 'text', 'preprocess', 'essenti', 'step', 'project', 'involv', 'nlp', 'sentiment', 'analysi', 'topic', 'model', 'text', 'classif', 'whether', 'deal', 'singl', 'paragraph', 'entir', 'corpu', 'preprocess', 'lie', 'groundwork', 'meaning', 'analysi']\n",
      "\n",
      "2 Dictionary: {'algorithm': 0, 'alphabet': 1, 'analysi': 2, 'analyz': 3, 'anoth': 4, 'brown': 5, 'classif': 6, 'contain': 7, 'corpu': 8, 'data': 9, 'datum': 10, 'deal': 11, 'demonstr': 12, 'dog': 13, 'easy': 14, 'english': 15, 'entir': 16, 'essenti': 17, 'everi': 18, 'exercis': 19, 'find': 20, 'font': 21, 'fox': 22, 'groundwork': 23, 'help': 24, 'howev': 25, 'involv': 26, 'jump': 27, 'languag': 28, 'lazi': 29, 'learn': 30, 'lemmat': 31, 'letter': 32, 'lie': 33, 'machin': 34, 'make': 35, 'meaning': 36, 'model': 37, 'natur': 38, 'nlp': 39, 'often': 40, 'paragraph': 41, 'pattern': 42, 'predict': 43, 'preprocess': 44, 'process': 45, 'project': 46, 'quick': 47, 'realm': 48, 'remov': 49, 'scientist': 50, 'sentenc': 51, 'sentiment': 52, 'simplifi': 53, 'singl': 54, 'stem': 55, 'step': 56, 'stopword': 57, 'text': 58, 'topic': 59, 'type': 60, 'use': 61, 'whether': 62}\n",
      "\n",
      "2 Corpus: [[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 3), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 4), (59, 1), (60, 1), (61, 1), (62, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RISHITHAA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    doc = nlp(\" \".join(tokens_stemmed))\n",
    "    tokens_lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "    return tokens_lemmatized\n",
    "\n",
    "def create_gensim_corpus(tokens_lemmatized):\n",
    "\n",
    "    dictionary = corpora.Dictionary([tokens_lemmatized])\n",
    "\n",
    "    corpus = [dictionary.doc2bow(tokens_lemmatized)]\n",
    "    \n",
    "    return dictionary, corpus\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\RISHITHAA\\Documents\\sample_text.txt\"\n",
    "  \n",
    "    preprocessed_tokens = preprocess_text(file_path)\n",
    "    \n",
    " \n",
    "    dictionary, corpus = create_gensim_corpus(preprocessed_tokens)\n",
    "\n",
    "    print(\"\\n2 Preprocessed Tokens:\", preprocessed_tokens)\n",
    "    print(\"\\n2 Dictionary:\", dictionary.token2id)\n",
    "    print(\"\\n2 Corpus:\", corpus)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def preprocess_text(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "      \n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: Unable to access '{file_path}'. Check file permissions.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: '{file_path}'\")\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a1c73",
   "metadata": {},
   "source": [
    "### Day-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30215e90",
   "metadata": {},
   "source": [
    "### Tokenizes a sample paragraph into words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f74eb3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "['\\nPython is an easy-to-learn, powerful programming language.', 'It has efficient high-level data structures and a simple but effective approach to object-oriented programming.', \"Python's simple structure and readability make it an excellent choice for beginners.\"]\n",
      "\n",
      "Words:\n",
      "['Python', 'is', 'an', 'easy-to-learn', ',', 'powerful', 'programming', 'language', '.', 'It', 'has', 'efficient', 'high-level', 'data', 'structures', 'and', 'a', 'simple', 'but', 'effective', 'approach', 'to', 'object-oriented', 'programming', '.', 'Python', \"'s\", 'simple', 'structure', 'and', 'readability', 'make', 'it', 'an', 'excellent', 'choice', 'for', 'beginners', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RISHITHAA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "paragraph = \"\"\"\n",
    "Python is an easy-to-learn, powerful programming language. \n",
    "It has efficient high-level data structures and a simple but effective approach to object-oriented programming. \n",
    "Python's simple structure and readability make it an excellent choice for beginners.\n",
    "\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "print(sentences)\n",
    "print(\"\\nWords:\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6767e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
